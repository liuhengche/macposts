{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNM_nb_folder = os.path.join('..', '..', '..', 'side_project', 'network_builder')\n",
    "sys.path.append(MNM_nb_folder)\n",
    "python_lib_folder = os.path.join('..', '..', 'pylib')\n",
    "sys.path.append(python_lib_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MNM_nb import *\n",
    "import MNMAPI\n",
    "from DODE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join('..', '..', '..', 'data', 'input_files_2link_fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNM_network_builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b10882e7e688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNM_network_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MNM_network_builder' is not defined"
     ]
    }
   ],
   "source": [
    "nb = MNM_network_builder()\n",
    "nb.load_from_folder(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d92c00049aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_lisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nb' is not defined"
     ]
    }
   ],
   "source": [
    "nb.link_lisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['use_link_flow'] = True\n",
    "config['use_link_tt'] = False\n",
    "config['link_flow_weight'] = 1\n",
    "config['link_tt_weight'] = 1\n",
    "config['num_data'] = 1\n",
    "config['observed_links'] = [3]\n",
    "config['paths_list'] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real = np.random.rand(10) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "link_df = pd.DataFrame(index = range(10), columns = [3], data = x_real)\n",
    "data_dict['link_flow'] = [link_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.463512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.434778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.338295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.372635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.430133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.417450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.953019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.435557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.198736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.266298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          3\n",
       "0  8.463512\n",
       "1  5.434778\n",
       "2  6.338295\n",
       "3  8.372635\n",
       "4  3.430133\n",
       "5  6.417450\n",
       "6  4.953019\n",
       "7  3.435557\n",
       "8  4.198736\n",
       "9  3.266298"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode = DODE(nb, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode.add_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode.estimate_path_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join('..', '..', '..', 'data', 'input_files_SR41_fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNM_config\n",
      "MNM_pathtable\n"
     ]
    }
   ],
   "source": [
    "nb = MNM_network_builder()\n",
    "nb.load_from_folder(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DTA]\n",
       "network_name = Snap_graph\n",
       "total_interval = 3000\n",
       "unit_time = 5\n",
       "assign_frq = 180\n",
       "start_assign_interval = 0\n",
       "max_interval = 6\n",
       "flow_scalar = 10\n",
       "num_of_link = 2065\n",
       "num_of_node = 1441\n",
       "num_of_O = 174\n",
       "num_of_D = 174\n",
       "OD_pair = 7110\n",
       "routing_type = Fixed\n",
       "\n",
       "[STAT]\n",
       "rec_mode = LRn\n",
       "rec_mode_para = 2\n",
       "rec_folder = record\n",
       "rec_volume = 0\n",
       "volume_load_automatic_rec = 0\n",
       "volume_record_automatic_rec = 0\n",
       "rec_tt = 0\n",
       "tt_load_automatic_rec = 0\n",
       "tt_record_automatic_rec = 0\n",
       "\n",
       "[FIXED]\n",
       "path_file_name = path_table\n",
       "num_path = 39561\n",
       "choice_portion = Buffer\n",
       "route_frq = 180"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_observable_list = list(map(lambda x: x.ID, filter(lambda x: x.ffs < 1000, nb.link_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obserable_list = list(filter(lambda x: np.random.rand() < 0.2, whole_observable_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_list = range(39561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real = np.random.rand(nb.config.config_dict['DTA']['max_interval'], len(obserable_list) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "link_df = pd.DataFrame(index = range(nb.config.config_dict['DTA']['max_interval']), \n",
    "                       columns = obserable_list, data = x_real)\n",
    "data_dict['link_flow'] = [link_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['use_link_flow'] = True\n",
    "config['use_link_tt'] = False\n",
    "config['link_flow_weight'] = 1\n",
    "config['link_tt_weight'] = 1\n",
    "config['num_data'] = 1\n",
    "config['observed_links'] = obserable_list\n",
    "config['paths_list'] = paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode = DODE(nb, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode.add_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 0 Loss: 3780.90498206\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 1 Loss: 5007.8498724\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 2 Loss: 3548.28220128\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 3 Loss: 2731.34717881\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 4 Loss: 2842.53829591\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 5 Loss: 2290.55251934\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 6 Loss: 2106.47158477\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 7 Loss: 2202.71715771\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 8 Loss: 2047.21351454\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 9 Loss: 1967.1387635\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 10 Loss: 1918.12755441\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 11 Loss: 1891.81365496\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 12 Loss: 1850.84636545\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 13 Loss: 1841.95057669\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 14 Loss: 1826.11789883\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 15 Loss: 1812.85156245\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 16 Loss: 1827.95427967\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 17 Loss: 1803.90753095\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 18 Loss: 1788.80343799\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 19 Loss: 1794.12399981\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 20 Loss: 1779.68345562\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 21 Loss: 1771.10191771\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 22 Loss: 1765.44015585\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 23 Loss: 1761.23211521\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 24 Loss: 1758.27628651\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 25 Loss: 1754.83640551\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 26 Loss: 1757.4467236\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 27 Loss: 1750.70948602\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 28 Loss: 1748.35651761\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 29 Loss: 1747.36099444\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 30 Loss: 1750.56961227\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 31 Loss: 1745.56560082\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 32 Loss: 1745.58835638\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 33 Loss: 1740.69856933\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 34 Loss: 1740.10121775\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 35 Loss: 1739.51990139\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 36 Loss: 1739.11538865\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 37 Loss: 1737.27996504\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 38 Loss: 1736.74143168\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 39 Loss: 1735.64794691\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 40 Loss: 1733.67506112\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 41 Loss: 1731.74557038\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 42 Loss: 1731.0852993\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 43 Loss: 1730.0873923\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 44 Loss: 1729.56307387\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 45 Loss: 1774.60675038\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 46 Loss: 1758.50611128\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 47 Loss: 1749.45876499\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 48 Loss: 1743.6868601\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 49 Loss: 1741.95564021\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 50 Loss: 1736.52005182\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 51 Loss: 1734.13486002\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 52 Loss: 1731.18285662\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 53 Loss: 1729.80746199\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 54 Loss: 1728.38229115\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 55 Loss: 1727.68465566\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 56 Loss: 1727.13067328\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 57 Loss: 1730.23386383\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 58 Loss: 1725.19659255\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 59 Loss: 1723.76732296\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 60 Loss: 1723.66748857\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 61 Loss: 1916.06013169\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 62 Loss: 1897.46803467\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 63 Loss: 1889.45654816\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 64 Loss: 1883.53591362\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 65 Loss: 1874.59179226\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 66 Loss: 1863.84153923\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 67 Loss: 1850.87883791\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 68 Loss: 1846.08848638\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 69 Loss: 1834.66874306\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 70 Loss: 1826.33696579\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 71 Loss: 1818.77968946\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 72 Loss: 1811.97094637\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 73 Loss: 1808.47862925\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 74 Loss: 1800.78440861\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 75 Loss: 1797.63534148\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 76 Loss: 1794.04880676\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 77 Loss: 1792.1206624\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 78 Loss: 1791.09340355\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 79 Loss: 1788.73521205\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 80 Loss: 1786.43521069\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 81 Loss: 1786.22552335\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 82 Loss: 1777.96367612\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 83 Loss: 1775.59760448\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 84 Loss: 1783.24034828\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 85 Loss: 1780.46197675\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 86 Loss: 1783.13293347\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 87 Loss: 1781.73606536\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 88 Loss: 1778.65724234\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 89 Loss: 1776.8651042\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 90 Loss: 1773.40285125\n",
      "Running simulation\n",
      "Getting DAR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 91 Loss: 1769.39890466\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 92 Loss: 1765.85314941\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 93 Loss: 1762.89677841\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 94 Loss: 1760.08144618\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 95 Loss: 1804.27430668\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 96 Loss: 1792.81309842\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 97 Loss: 1782.59600693\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 98 Loss: 1776.05313172\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 99 Loss: 1766.74675393\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 100 Loss: 1763.47057685\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 101 Loss: 1754.89105991\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 102 Loss: 1750.35690398\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 103 Loss: 1746.5226212\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 104 Loss: 1742.86217254\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 105 Loss: 1739.60640396\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 106 Loss: 1739.15960116\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 107 Loss: 1736.13233927\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 108 Loss: 1732.72218153\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 109 Loss: 1729.77676282\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 110 Loss: 1728.59533805\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 111 Loss: 1726.31989418\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 112 Loss: 1725.37614938\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 113 Loss: 1723.87014921\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 114 Loss: 1722.33848757\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 115 Loss: 1721.30539057\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 116 Loss: 1720.75138879\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 117 Loss: 1720.18227692\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 118 Loss: 1719.86876204\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 119 Loss: 1719.27070582\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 120 Loss: 1719.2898849\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 121 Loss: 1717.84839163\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 122 Loss: 1717.98873222\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 123 Loss: 1716.81345635\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 124 Loss: 1717.02069296\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 125 Loss: 1716.8744928\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 126 Loss: 1715.80025765\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 127 Loss: 1715.3849892\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 128 Loss: 1715.36113551\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 129 Loss: 1715.81237007\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 130 Loss: 1715.06912037\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 131 Loss: 1714.44113042\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 132 Loss: 1714.49244296\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 133 Loss: 1714.30515456\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 134 Loss: 1713.84982869\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 135 Loss: 1714.2876619\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 136 Loss: 1717.74245105\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 137 Loss: 1713.89594653\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 138 Loss: 1713.81271763\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 139 Loss: 1712.88061291\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 140 Loss: 1713.34241797\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 141 Loss: 1713.81998313\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 142 Loss: 1713.58592327\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 143 Loss: 1713.08759196\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 144 Loss: 1712.67134719\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 145 Loss: 1713.28451861\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 146 Loss: 1712.65221509\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 147 Loss: 1712.56710445\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 148 Loss: 1712.43313736\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 149 Loss: 1712.2285872\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 150 Loss: 1712.26874614\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 151 Loss: 1712.31583924\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 152 Loss: 1713.13238549\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 153 Loss: 1712.18305598\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 154 Loss: 1712.6243257\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 155 Loss: 1716.17524969\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 156 Loss: 1711.52455595\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 157 Loss: 1711.3897086\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 158 Loss: 1711.50673637\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 159 Loss: 1711.54554519\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 160 Loss: 1711.66152709\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 161 Loss: 1711.76784882\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 162 Loss: 1711.51181184\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 163 Loss: 1711.53450386\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 164 Loss: 1715.15786267\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 165 Loss: 1711.33979092\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 166 Loss: 1711.88775259\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 167 Loss: 1711.15614963\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 168 Loss: 1711.23623597\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 169 Loss: 1711.07239558\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 170 Loss: 1711.0526638\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 171 Loss: 1711.21647192\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 172 Loss: 1711.30874637\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 173 Loss: 1714.16931173\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 174 Loss: 1710.92993056\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 175 Loss: 1711.09697756\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 176 Loss: 1714.94284513\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 177 Loss: 1711.05966336\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 178 Loss: 1713.03776147\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 179 Loss: 1710.83504996\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 180 Loss: 1710.90784517\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 181 Loss: 1710.91959048\n",
      "Running simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 182 Loss: 1710.81506084\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 183 Loss: 1710.56988578\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 184 Loss: 1710.7414446\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 185 Loss: 1711.14570271\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 186 Loss: 1710.7902795\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 187 Loss: 1710.75026736\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 188 Loss: 1711.12105676\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 189 Loss: 1710.60812663\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 190 Loss: 1710.06262968\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 191 Loss: 1710.23898086\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 192 Loss: 1709.97283744\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 193 Loss: 1710.40521201\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 194 Loss: 1710.44482538\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 195 Loss: 1710.27505123\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 196 Loss: 1710.18023515\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 197 Loss: 1713.8608966\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 198 Loss: 1711.20498042\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 199 Loss: 1710.4471841\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 200 Loss: 1710.14641713\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 201 Loss: 1710.09893716\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 202 Loss: 1709.87788113\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 203 Loss: 1710.07998416\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 204 Loss: 1710.137775\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 205 Loss: 1710.08054472\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 206 Loss: 1709.96374077\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 207 Loss: 1710.06128493\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 208 Loss: 1710.27892793\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 209 Loss: 1710.65596662\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 210 Loss: 1710.6748654\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 211 Loss: 1711.13198926\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 212 Loss: 1710.15888522\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 213 Loss: 1710.30825697\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 214 Loss: 1712.80321734\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 215 Loss: 1712.16716583\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 216 Loss: 1710.2708146\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 217 Loss: 1709.78599907\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 218 Loss: 1710.22446321\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 219 Loss: 1712.65249607\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 220 Loss: 1710.03179603\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 221 Loss: 1709.76739878\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 222 Loss: 1709.8602854\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 223 Loss: 1709.94515224\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 224 Loss: 1713.14407474\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 225 Loss: 1710.46466662\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 226 Loss: 1709.43682673\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 227 Loss: 1711.90825772\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 228 Loss: 1710.77056232\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 229 Loss: 1710.39986164\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 230 Loss: 1709.85700339\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 231 Loss: 1711.69047912\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 232 Loss: 1709.92525586\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 233 Loss: 1709.5704846\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 234 Loss: 1709.79363836\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 235 Loss: 1709.45294166\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 236 Loss: 1709.56664225\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 237 Loss: 1709.67138205\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 238 Loss: 1709.9109149\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 239 Loss: 1709.47696617\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 240 Loss: 1710.16051054\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 241 Loss: 1709.17279522\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 242 Loss: 1709.45170953\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 243 Loss: 1709.03497106\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 244 Loss: 1709.815363\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 245 Loss: 1709.47360702\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 246 Loss: 1710.15214361\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 247 Loss: 1709.77120969\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 248 Loss: 1709.5231826\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 249 Loss: 1709.1641354\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 250 Loss: 1709.7342387\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 251 Loss: 1709.25660092\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 252 Loss: 1709.39413084\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 253 Loss: 1709.04652751\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 254 Loss: 1709.3848322\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 255 Loss: 1708.71696062\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 256 Loss: 1713.12501639\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 257 Loss: 1708.97812007\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 258 Loss: 1709.15412188\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 259 Loss: 1708.78375962\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 260 Loss: 1708.98074834\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 261 Loss: 1709.18233087\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 262 Loss: 1709.41852564\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 263 Loss: 1709.29683261\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 264 Loss: 1708.86039761\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 265 Loss: 1709.76533349\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 266 Loss: 1709.09440016\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 267 Loss: 1709.39045505\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 268 Loss: 1709.6448764\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 269 Loss: 1709.19656288\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 270 Loss: 1709.05072781\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 271 Loss: 1709.16089972\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 272 Loss: 1709.09255303\n",
      "Running simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 273 Loss: 1709.14533173\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 274 Loss: 1709.1461095\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 275 Loss: 1708.82585832\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 276 Loss: 1712.51314659\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 277 Loss: 1708.97554179\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 278 Loss: 1709.390509\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 279 Loss: 1708.52708974\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 280 Loss: 1708.68377978\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 281 Loss: 1709.31285565\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 282 Loss: 1708.81344289\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 283 Loss: 1709.52425634\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 284 Loss: 1709.36983936\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 285 Loss: 1708.85848841\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 286 Loss: 1709.46408976\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 287 Loss: 1711.46872209\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 288 Loss: 1709.43881237\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 289 Loss: 1709.88293308\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 290 Loss: 1709.26902237\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 291 Loss: 1708.83757214\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 292 Loss: 1708.91179131\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 293 Loss: 1709.25357111\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 294 Loss: 1709.30262325\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 295 Loss: 1709.09284242\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 296 Loss: 1709.68173222\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 297 Loss: 1709.14529154\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 298 Loss: 1711.53877963\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 299 Loss: 1711.36635527\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 300 Loss: 1710.8346101\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 301 Loss: 1710.83107542\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 302 Loss: 1710.00887967\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 303 Loss: 1709.88372306\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 304 Loss: 1709.73927615\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 305 Loss: 1713.37850902\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 306 Loss: 1709.31821559\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 307 Loss: 1709.35152483\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 308 Loss: 1709.4015204\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 309 Loss: 1709.45094647\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 310 Loss: 1709.03400587\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 311 Loss: 1709.83723582\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 312 Loss: 1709.33597555\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 313 Loss: 1709.23291657\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 314 Loss: 1709.0876454\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 315 Loss: 1708.50129547\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 316 Loss: 1708.98445141\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 317 Loss: 1708.88726065\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 318 Loss: 1709.60308597\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 319 Loss: 1708.77531105\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 320 Loss: 1709.09464157\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 321 Loss: 1712.03181262\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 322 Loss: 1708.69635233\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 323 Loss: 1708.87969743\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 324 Loss: 1708.76180672\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 325 Loss: 1708.86903268\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 326 Loss: 1708.41125199\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 327 Loss: 1708.55014947\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 328 Loss: 1708.72108287\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 329 Loss: 1708.52852616\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 330 Loss: 1708.55943528\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 331 Loss: 1708.38381627\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 332 Loss: 1708.18665358\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 333 Loss: 1708.77382147\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 334 Loss: 1708.63827112\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 335 Loss: 1708.43557772\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 336 Loss: 1709.15570361\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 337 Loss: 1707.94552824\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 338 Loss: 1708.14612985\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 339 Loss: 1708.42065375\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 340 Loss: 1708.17803631\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 341 Loss: 1709.10554221\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 342 Loss: 1708.38987065\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 343 Loss: 1708.59639177\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 344 Loss: 1708.29610681\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 345 Loss: 1708.53153476\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 346 Loss: 1708.7157027\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 347 Loss: 1708.46312326\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 348 Loss: 1708.36840774\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 349 Loss: 1708.35728079\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 350 Loss: 1708.07705862\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 351 Loss: 1708.35486118\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 352 Loss: 1708.79611444\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 353 Loss: 1708.67507334\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 354 Loss: 1708.97803634\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 355 Loss: 1708.8351307\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 356 Loss: 1708.46852757\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 357 Loss: 1708.32760496\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 358 Loss: 1708.25860321\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 359 Loss: 1712.5943465\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 360 Loss: 1708.02955162\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 361 Loss: 1708.10896995\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 362 Loss: 1707.79975025\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 363 Loss: 1708.10034358\n",
      "Running simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 364 Loss: 1708.06968528\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 365 Loss: 1708.82540805\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 366 Loss: 1708.16248129\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 367 Loss: 1708.26478781\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 368 Loss: 1711.33713271\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 369 Loss: 1710.03950827\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 370 Loss: 1707.86203489\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 371 Loss: 1707.86208502\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 372 Loss: 1708.86102631\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 373 Loss: 1709.41795149\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 374 Loss: 1707.64515984\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 375 Loss: 1708.09111039\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 376 Loss: 1708.16636853\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 377 Loss: 1709.47681344\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 378 Loss: 1707.90299399\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 379 Loss: 1708.34105512\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 380 Loss: 1707.88066188\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 381 Loss: 1708.07218703\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 382 Loss: 1708.85483327\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 383 Loss: 1708.54551463\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 384 Loss: 1707.94677921\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 385 Loss: 1707.6261981\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 386 Loss: 1708.38919081\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 387 Loss: 1707.73062569\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 388 Loss: 1760.0260817\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 389 Loss: 1755.76419439\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 390 Loss: 1753.4189382\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 391 Loss: 1756.09878778\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 392 Loss: 1753.81347302\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 393 Loss: 1757.41741823\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 394 Loss: 1752.29466528\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 395 Loss: 1748.44061557\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 396 Loss: 1751.32531237\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 397 Loss: 1751.81590736\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 398 Loss: 1748.41156017\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 399 Loss: 1750.79565536\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 400 Loss: 1749.51951917\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 401 Loss: 1749.11856391\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 402 Loss: 1747.62022179\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 403 Loss: 1748.12490142\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 404 Loss: 1743.88397602\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 405 Loss: 1745.78711764\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 406 Loss: 1745.88895456\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 407 Loss: 1743.24991704\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 408 Loss: 1742.2674222\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 409 Loss: 1739.37216102\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 410 Loss: 1738.3714524\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 411 Loss: 1737.12766142\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 412 Loss: 1735.67583752\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 413 Loss: 1734.41043773\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 414 Loss: 1733.15950874\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 415 Loss: 1735.23732473\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 416 Loss: 1734.75297105\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 417 Loss: 1729.57246431\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 418 Loss: 1728.77408904\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 419 Loss: 1727.33544392\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 420 Loss: 1729.09246152\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 421 Loss: 1725.18264168\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 422 Loss: 1723.88950789\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 423 Loss: 1723.18981487\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 424 Loss: 1721.97954504\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 425 Loss: 1721.61673147\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 426 Loss: 1723.87891408\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 427 Loss: 1720.60867321\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 428 Loss: 1719.23183575\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 429 Loss: 1718.53874683\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 430 Loss: 1718.72289286\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 431 Loss: 1717.77931966\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 432 Loss: 1716.53337359\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 433 Loss: 1716.60446202\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 434 Loss: 1715.19922289\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 435 Loss: 1715.59872309\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 436 Loss: 1714.66109154\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 437 Loss: 1714.1507122\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 438 Loss: 1713.56372162\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 439 Loss: 1713.24599258\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 440 Loss: 1713.40748112\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 441 Loss: 1712.84987028\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 442 Loss: 1711.8853026\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 443 Loss: 1711.84273502\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 444 Loss: 1712.0472439\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 445 Loss: 1711.38858848\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 446 Loss: 1710.95794765\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 447 Loss: 1710.76049071\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 448 Loss: 1710.75900507\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 449 Loss: 1710.72268986\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 450 Loss: 1710.49464148\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 451 Loss: 1709.70977348\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 452 Loss: 1712.82446789\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 453 Loss: 1709.08069126\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 454 Loss: 1709.65231935\n",
      "Running simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 455 Loss: 1708.71083988\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 456 Loss: 1712.15479645\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 457 Loss: 1708.432285\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 458 Loss: 1708.86310416\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 459 Loss: 1707.58147514\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 460 Loss: 1707.52024337\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 461 Loss: 1708.16295464\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 462 Loss: 1707.64902323\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 463 Loss: 1707.85499641\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 464 Loss: 1707.19660307\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 465 Loss: 1707.05109814\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 466 Loss: 1707.28810747\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 467 Loss: 1707.12231227\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 468 Loss: 1706.64233931\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 469 Loss: 1706.72267958\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 470 Loss: 1706.87801135\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 471 Loss: 1707.52139694\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 472 Loss: 1707.02169514\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 473 Loss: 1706.66624652\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 474 Loss: 1706.32263314\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 475 Loss: 1706.86461645\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 476 Loss: 1706.20041569\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 477 Loss: 1709.27415719\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 478 Loss: 1706.02038895\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 479 Loss: 1707.29426843\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 480 Loss: 1706.07932941\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 481 Loss: 1705.67119822\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 482 Loss: 1705.90790635\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 483 Loss: 1706.47835578\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 484 Loss: 1705.69285911\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 485 Loss: 1705.79175316\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 486 Loss: 1707.98848719\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 487 Loss: 1705.6500784\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 488 Loss: 1705.67983569\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 489 Loss: 1706.32655733\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 490 Loss: 1705.93390227\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 491 Loss: 1706.24886694\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 492 Loss: 1705.50465679\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 493 Loss: 1705.47091417\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 494 Loss: 1705.78765291\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 495 Loss: 1705.29678931\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 496 Loss: 1707.77936631\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 497 Loss: 1705.31344855\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 498 Loss: 1705.63788089\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 499 Loss: 1705.75290423\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 500 Loss: 1705.08571025\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 501 Loss: 1705.51578013\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 502 Loss: 1705.44441604\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 503 Loss: 1705.81110264\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 504 Loss: 1708.55216939\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 505 Loss: 1707.56938716\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 506 Loss: 1704.84093022\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 507 Loss: 1705.35888419\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 508 Loss: 1705.11047693\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 509 Loss: 1705.09619427\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 510 Loss: 1704.98869888\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 511 Loss: 1705.07370015\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 512 Loss: 1705.223226\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 513 Loss: 1704.85463133\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 514 Loss: 1704.96880153\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 515 Loss: 1705.08212945\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 516 Loss: 1705.62074689\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 517 Loss: 1705.6649146\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 518 Loss: 1704.7540684\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 519 Loss: 1704.92019734\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 520 Loss: 1704.87476455\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 521 Loss: 1704.71359044\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 522 Loss: 1706.22538004\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 523 Loss: 1707.95273105\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 524 Loss: 1704.47229347\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 525 Loss: 1705.13396969\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 526 Loss: 1704.89409191\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 527 Loss: 1704.57750082\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 528 Loss: 1704.84872024\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 529 Loss: 1705.11921745\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 530 Loss: 1705.12079292\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 531 Loss: 1704.92281689\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 532 Loss: 1705.96355988\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 533 Loss: 1705.41464707\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 534 Loss: 1708.44623623\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 535 Loss: 1705.94036663\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 536 Loss: 1704.96597914\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 537 Loss: 1704.4869453\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 538 Loss: 1704.85576353\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 539 Loss: 1706.23643087\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 540 Loss: 1705.102273\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 541 Loss: 1707.43848078\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 542 Loss: 1705.33909197\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 543 Loss: 1704.67790892\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 544 Loss: 1705.61198836\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 545 Loss: 1704.50883016\n",
      "Running simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 546 Loss: 1705.75487191\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 547 Loss: 1704.84613987\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 548 Loss: 1705.17366309\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 549 Loss: 1707.28299122\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 550 Loss: 1704.91844316\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 551 Loss: 1704.77563972\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 552 Loss: 1704.8762037\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 553 Loss: 1705.66694166\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 554 Loss: 1704.88326001\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 555 Loss: 1704.89396871\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 556 Loss: 1706.10942443\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 557 Loss: 1704.96629279\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 558 Loss: 1704.88614132\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 559 Loss: 1705.30515686\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 560 Loss: 1704.61834838\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 561 Loss: 1707.2812523\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 562 Loss: 1704.91539924\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 563 Loss: 1705.1184679\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 564 Loss: 1704.84582698\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 565 Loss: 1707.12563323\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 566 Loss: 1705.00657543\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 567 Loss: 1704.81819123\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 568 Loss: 1704.74147828\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 569 Loss: 1704.52579049\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 570 Loss: 1704.79558632\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 571 Loss: 1704.84686138\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 572 Loss: 1705.27995835\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 573 Loss: 1704.67405319\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 574 Loss: 1705.16029762\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 575 Loss: 1705.55706393\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 576 Loss: 1704.61552188\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 577 Loss: 1704.56555335\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 578 Loss: 1704.82510206\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 579 Loss: 1705.50213075\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 580 Loss: 1704.32208355\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 581 Loss: 1704.73906619\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 582 Loss: 1704.13873833\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 583 Loss: 1704.82706352\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 584 Loss: 1704.71155824\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 585 Loss: 1704.32829546\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 586 Loss: 1704.75873856\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 587 Loss: 1704.7728933\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 588 Loss: 1704.77392677\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 589 Loss: 1704.93616202\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 590 Loss: 1706.57946608\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 591 Loss: 1704.76449112\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 592 Loss: 1707.48049338\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 593 Loss: 1705.10781407\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 594 Loss: 1704.45270796\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 595 Loss: 1705.09672739\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 596 Loss: 1705.10351253\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 597 Loss: 1704.57796771\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 598 Loss: 1704.8656857\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 599 Loss: 1705.33609154\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 600 Loss: 1705.22434914\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 601 Loss: 1704.62967445\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 602 Loss: 1707.95046541\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 603 Loss: 1704.78102643\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 604 Loss: 1704.79588544\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 605 Loss: 1704.85257227\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 606 Loss: 1704.17320637\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 607 Loss: 1704.92284506\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 608 Loss: 1704.60538019\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 609 Loss: 1704.93453327\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 610 Loss: 1704.74931324\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 611 Loss: 1704.79406064\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 612 Loss: 1704.34790711\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 613 Loss: 1705.01728737\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 614 Loss: 1704.51275272\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 615 Loss: 1704.53484681\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 616 Loss: 1708.09608458\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 617 Loss: 1704.56028194\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 618 Loss: 1704.48208129\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 619 Loss: 1704.49682134\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 620 Loss: 1704.5613243\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 621 Loss: 1704.24119095\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 622 Loss: 1704.29857863\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 623 Loss: 1704.61726757\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 624 Loss: 1707.43037842\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 625 Loss: 1704.32044878\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 626 Loss: 1704.70266326\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 627 Loss: 1705.92923545\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 628 Loss: 1704.63326006\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 629 Loss: 1704.39844165\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 630 Loss: 1704.73388201\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 631 Loss: 1705.10609802\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 632 Loss: 1707.52757941\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 633 Loss: 1704.5536965\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 634 Loss: 1705.15900367\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 635 Loss: 1707.04996346\n",
      "Running simulation\n",
      "Getting DAR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 636 Loss: 1704.75327791\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 637 Loss: 1704.41584726\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 638 Loss: 1707.21618285\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 639 Loss: 1705.10743665\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 640 Loss: 1704.3859025\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 641 Loss: 1704.67604848\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 642 Loss: 1704.23367692\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 643 Loss: 1705.39800904\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 644 Loss: 1704.58304109\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 645 Loss: 1705.78729706\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 646 Loss: 1704.7460964\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 647 Loss: 1705.27295909\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 648 Loss: 1704.384067\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 649 Loss: 1706.23741943\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 650 Loss: 1704.67678698\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 651 Loss: 1704.55753301\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 652 Loss: 1704.93600154\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 653 Loss: 1704.73520651\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 654 Loss: 1708.33314764\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 655 Loss: 1704.69631146\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 656 Loss: 1704.84759804\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 657 Loss: 1704.48750253\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 658 Loss: 1704.2678697\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 659 Loss: 1704.28691558\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 660 Loss: 1704.33804301\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 661 Loss: 1704.71563098\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 662 Loss: 1704.73153908\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 663 Loss: 1704.69469547\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 664 Loss: 1705.30538191\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 665 Loss: 1704.39080653\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 666 Loss: 1704.72700663\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 667 Loss: 1704.21164269\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 668 Loss: 1705.24561295\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 669 Loss: 1704.57757861\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 670 Loss: 1704.39433467\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 671 Loss: 1707.65307835\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 672 Loss: 1704.90145124\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 673 Loss: 1705.20292638\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 674 Loss: 1704.69451995\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 675 Loss: 1704.84736742\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 676 Loss: 1704.82762634\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 677 Loss: 1704.53466716\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 678 Loss: 1707.02338392\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 679 Loss: 1707.12172263\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 680 Loss: 1706.43438892\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 681 Loss: 1705.74745545\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 682 Loss: 1704.61062307\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 683 Loss: 1704.70654475\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 684 Loss: 1704.51953995\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 685 Loss: 1704.62986555\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 686 Loss: 1705.37568452\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 687 Loss: 1704.84393077\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 688 Loss: 1705.49507252\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 689 Loss: 1704.66939891\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 690 Loss: 1705.3083532\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 691 Loss: 1704.55366169\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 692 Loss: 1705.42706654\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 693 Loss: 1705.08686911\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 694 Loss: 1704.33017795\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 695 Loss: 1704.26624894\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 696 Loss: 1705.23904153\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 697 Loss: 1706.0288973\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 698 Loss: 1704.52745352\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 699 Loss: 1704.68273299\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 700 Loss: 1704.53216381\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 701 Loss: 1704.49528746\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 702 Loss: 1704.39179298\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 703 Loss: 1704.60137134\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 704 Loss: 1704.42450658\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 705 Loss: 1704.79031413\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 706 Loss: 1704.13722284\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 707 Loss: 1707.3158121\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 708 Loss: 1705.55909117\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 709 Loss: 1704.20281467\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 710 Loss: 1705.33680971\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 711 Loss: 1704.09622821\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 712 Loss: 1704.5491831\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 713 Loss: 1704.47577457\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 714 Loss: 1704.60283277\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 715 Loss: 1704.53068271\n",
      "Running simulation\n",
      "Getting DAR\n",
      "Evaluating grad\n",
      "Getting Loss\n",
      "Epoch: 716 Loss: 1704.73218875\n",
      "Running simulation\n"
     ]
    }
   ],
   "source": [
    "dode.estimate_path_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_e = dode.init_path_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash = hashlib.sha1()\n",
    "hash.update(str(time.time()))\n",
    "new_folder = str(hash.hexdigest())\n",
    "nb.update_demand_path(f_e)\n",
    "nb.dump_to_folder(new_folder)\n",
    "a = MNMAPI.dta_api()\n",
    "a.initialize(new_folder)\n",
    "# shutil.rmtree(new_folder)\n",
    "a.register_links(obserable_list)\n",
    "a.register_paths(paths_list)\n",
    "a.install_cc()\n",
    "a.install_cc_tree()\n",
    "a.run_whole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a499a2d6a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nb' is not defined"
     ]
    }
   ],
   "source": [
    "nb.demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dode = DODE(nb, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_e = dode.init_path_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = dode._run_simulation(f_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = dta.get_link_tt(np.arange(0, dode.num_loading_interval, dode.ass_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link_flow_weight': 1,\n",
       " 'link_tt_weight': 1,\n",
       " 'num_data': 1,\n",
       " 'observed_links': [3],\n",
       " 'paths_list': [0, 1],\n",
       " 'use_link_flow': True,\n",
       " 'use_link_tt': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  10.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.get_link(3).get_fft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f3ac46944ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lemma/anaconda2/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m   \u001b[0;31m# spmatrix will check for errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemma/anaconda2/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m\"\"\"See `reshape`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x5 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
